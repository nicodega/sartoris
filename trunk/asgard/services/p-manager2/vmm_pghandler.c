/*
*
*	Copyright (C) 2002, 2003, 2004, 2005
*       
*	Santiago Bazerque 	sbazerque@gmail.com			
*	Nicolas de Galarreta	nicodega@gmail.com
*
*	
*	Redistribution and use in source and binary forms, with or without 
* 	modification, are permitted provided that conditions specified on 
*	the License file, located at the root project directory are met.
*
*	You should have received a copy of the License along with the code,
*	if not, it can be downloaded from our project site: sartoris.sourceforge.net,
*	or you can contact us directly at the email addresses provided above.
*
*
*/


#include <sartoris/kernel.h>
#include <sartoris/syscall.h>
#include "layout.h"
#include "vm.h"
#include "types.h"
#include "exception.h"
#include "scheduler.h"
#include "loader.h"
#include "task_thread.h"
#include "formats/ia32paging.h"
#include "rb.h"

/*
thr_id: If not NULL it will be set to the page fault rising thread id. (If its an internal call this MUST contain the thread id)
internal: If TRUE, handler was invoked by pman, FALSE if invoked by PF interrupt.
*/
BOOL vmm_handle_page_fault(UINT16 *thr_id, BOOL internal)
{
	struct page_fault pf;
	ADDR page_addr;
	UINT16 task_id, thread_id;
	struct pm_thread *curr_thr = NULL;
	struct pm_thread *thread = NULL;
	struct pm_task *task = NULL;
	struct vmm_page_table *optbl = NULL;
	struct vmm_page_directory *directory = NULL;
	UINT32 filepos, readsize;
	INT32 perms, page_displacement;

    /*
    FIX ME!!!

    Check if the page fault task/thread is the same as the one which generated the fault.
    It might not be if it was a thread from another task trying to access an SMO which was swapped!

    */

	if(!internal)
	{
        /* Fault was generated by PF interrupt */
		get_page_fault(&pf);
        
        /* is sartoris requesting/returning memory? */
        if(pf.flags != PF_FLAG_NONE && pf.flags != PF_FLAG_EXT)
        {
            if(pf.flags == PF_FLAG_FREE)
            {
                // sartoris is returning a page
                // last_page_fault.linear contains the physical address
                vmm_pm_put_page((ADDR)PHYSICAL2LINEAR(pf.linear));
            }
            else
            {
                thread = thr_get(pf.thread_id);

                // sartoris is requesting pages
                page_addr = vmm_pm_get_page(FALSE);

                if(page_addr == NULL)
                    pman_print_dbg("PMAN: NO FREE PAGE FOR SARTORIS");

                if(grant_page_mk(LINEAR2PHYSICAL(page_addr)) < 0)
                    pman_print_and_stop("PMAN: grant_page_mk failed!");
            }
            // don't put the thread on hold.
            return FALSE;
        }
        else
        {
		    if(pf.task_id == PMAN_TASK)
			    pman_print_and_stop("PMAN: INTERNAL PF linear: %x, task: %i, thr: %i ", pf.linear, pf.task_id, pf.thread_id);
		
		    task_id = pf.task_id; 
		    thread_id = pf.thread_id;
		
		    thread = thr_get(thread_id);
		    task = tsk_get(task_id);

		    if(thread == NULL || task == NULL)
			    pman_print_and_stop("PF: NULL TASK/THREAD");

		    if(thread->state == THR_INTHNDL)
			    pman_print_and_stop("INT HADLER PAGE FAULT!");
        		
		    thread->vmm_info.fault_entry.addr = 0;
		    thread->vmm_info.fault_entry.present = 0;
		    thread->vmm_info.fault_entry.swapped = 0;
		    thread->vmm_info.fault_entry.unused = 0;
            thread->vmm_info.fault_task = task_id;

		    if(thr_id != NULL) *thr_id = thread_id;

		    /* Check Page is not being fetched by other thread */
            thread->vmm_info.pg_node.value = PG_ADDRESS(pf.linear);
                        
            if(insertRedBlackTree(&task->vmm_info.wait_root, &thread->vmm_info.pg_node, TRUE) == 2)
            {
                pman_print_dbg("PF: Other Thread was waiting \n");
                sch_deactivate(thread);

			    /* Page was being retrieved already for other thread! Block the thread.	*/
			    thread->state = THR_BLOCKED;
			    thread->flags |= THR_FLAG_PAGEFAULT;
			    thread->vmm_info.fault_address = pf.linear;
			    thread->vmm_info.read_size = curr_thr->vmm_info.read_size;
			    thread->vmm_info.page_perms = curr_thr->vmm_info.page_perms;
			    thread->vmm_info.page_displacement = curr_thr->vmm_info.page_displacement;
			    thread->vmm_info.fault_task = task_id;

			    return TRUE;	// thread will remain on hold
		    }
        }
	}
	else
	{
		/* 
		Internal faults will be generated by the process manager itself when a 
		page table is fetched from swap file. 
        NOTE: If there was another thread waiting for the same page, this thread
        must already be on the tree, and will be handled when the page is fetched
        for that thread.
		*/
		thread = thr_get(*thr_id);

		if(thread->flags & THR_FLAG_PAGEFAULT)
		{
			/* Thread already is being handled, keep it blocked. */
			return TRUE;
		}

		task_id = thread->task_id; 
		thread_id = *thr_id;

		task = tsk_get(task_id);

		thread->vmm_info.fault_entry.addr = 0;
		thread->vmm_info.fault_entry.present = 0;
		thread->vmm_info.fault_entry.swapped = 0;
		thread->vmm_info.fault_entry.unused = 0;

		/* Build a bogus page fault */
		pf.linear = thread->vmm_info.fault_address;
		pf.task_id = task_id;
		pf.thread_id = thread_id;

	}
	
	thread->vmm_info.fault_address = pf.linear;

	/* Check PF is not above max_addr */
	if(task->vmm_info.max_addr <= (UINT32)pf.linear)
	{
		thread->state = THR_EXEPTION;
		sch_deactivate(thread);

        pman_print("MAX ADDR PF t: %i vadd: %x ", task->id, pf.linear);
		// FIXME: Should send an exception signal... 
		return TRUE;
	}

	/* Check if page table is on swap.
    NOTE: This can only happen when all pages had been sent to swap. The page table
    was also sent to swap because we cannot discard it, for it has swap addresses for it's pages.
    */
    
	if(!(task->flags & TSK_FLAG_SYS_SERVICE) 
        && vmm_check_swap_tbl(task, thread, (ADDR)PG_ADDRESS(pf.linear)))
	{
        pman_print_dbg("PF: Table is on SWAP \n");
        // we must insert the thread on the pg wait tree
        // in case another thread asks for the same page.
        return TRUE;
	}

	/* Lets see if the page table is present on the page directory and if not give it one */
	if(task->vmm_info.page_directory->tables[PM_LINEAR_TO_DIR(pf.linear)].ia32entry.present == 0)
	{
		/* Get a Page and set taken */
		page_addr = vmm_get_tblpage(task->id, PG_ADDRESS(pf.linear));
		
		/* Page in the table on task linear space. */
		pm_page_in(task->id, (ADDR)PG_ADDRESS(pf.linear), (ADDR)LINEAR2PHYSICAL(page_addr), 1, PGATT_WRITE_ENA);

		task->vmm_info.page_count++;
	}
	
	/* See if page is file mapped */
	if(vmm_page_filemapped(task, thread, (ADDR)PG_ADDRESS(pf.linear)))
	{
        pman_print_dbg("PF: Page is filemapped \n");
		// Page is file mapped and a read operation has begun. //
        return TRUE;
	}

	/* 
	If it's a shared page, see if it's paged in on the owner task.
	*/
	if(vmm_is_shared(task, (ADDR)PG_ADDRESS(pf.linear)))
	{
        pman_print_dbg("PF: Page is shared \n");
		ADDR owner_laddr = NULL;
		UINT32 attrib = 0;

		// Page is shared, check if it's present on the owner task. //
		struct pm_task *otsk = vmm_shared_getowner(task, (ADDR)PG_ADDRESS(pf.linear), &owner_laddr, &attrib);

		optbl = (struct vmm_page_table *)PHYSICAL2LINEAR(otsk->vmm_info.page_directory->tables[PM_LINEAR_TO_DIR(owner_laddr)].b);
		
		if(optbl->pages[PM_LINEAR_TO_TAB(owner_laddr)].entry.ia32entry.present == 0)
		{
			// We won't page out shared pages... if this happened we are screwed. //
		}
		else
		{
			// Map the page //
			pm_page_in(task->id, (ADDR)pf.linear, (ADDR)PG_ADDRESS(optbl->pages[PM_LINEAR_TO_TAB(owner_laddr)].entry.phy_page_addr), 2, attrib);
			return FALSE;
		}
	}
	

	/* If task is not a system service, check if page requested is on Swap. */
	if(!(task->flags & TSK_FLAG_SYS_SERVICE) 
        && vmm_check_swap(task, thread, (ADDR)PG_ADDRESS(pf.linear)) )
	{
        pman_print_dbg("PF: Page is swapped \n");
        // we must insert the thread on the pg wait tree
        // in case another thread asks for the same page.
        return TRUE;
	}
		
	directory = task->vmm_info.page_directory;

	/* Check if Page belongs to the executable file or is just data */
	filepos = 0;
	readsize = 0;
	page_displacement = 0;
	perms = PGATT_WRITE_ENA;

	/* Must page be read from elf file? (initial system services will be pre loaded) */
	if(!(task->flags & TSK_FLAG_SYS_SERVICE) 
        && loader_filepos(task, pf.linear, &filepos, &readsize, &perms, &page_displacement))
	{		
        /* Thread must be blocked until data is read from the executable file */
		sch_deactivate(thread);

		thread->state = THR_BLOCKED;
		thread->flags |= THR_FLAG_PAGEFAULT;
		thread->vmm_info.fault_address = pf.linear;
		thread->vmm_info.read_size = readsize;
		thread->vmm_info.page_perms = perms;
		thread->vmm_info.page_displacement = page_displacement;
        thread->vmm_info.fault_task = task_id;
        thread->vmm_info.pg_node.value = PG_ADDRESS(pf.linear);

		/* 
		Page will be granted before hand, so page stealing thread won't attempt to take our page table.
		We also set taken entry here.
		*/
		thread->vmm_info.page_in_address = vmm_get_page(task->id, PG_ADDRESS(pf.linear));

		task->vmm_info.page_count++;

		/* IO lock page table */
		vmm_set_flags(task_id, (ADDR)PHYSICAL2LINEAR(vmm_get_tbl_physical(task->id, pf.linear)), TRUE, TAKEN_EFLAG_IOLOCK | TAKEN_EFLAG_PF, TRUE);

		/* IO Lock Page */
		vmm_set_flags(task_id, thread->vmm_info.page_in_address, TRUE, TAKEN_EFLAG_IOLOCK, TRUE);

		thread->io_finished.callback = vmm_elffile_seekend_callback;
		
		io_begin_seek(&thread->io_event_src, filepos);

        // we must insert the thread on the pg wait tree
        // in case another thread asks for the same page.
        insertRedBlackTree(&task->vmm_info.wait_root, &thread->vmm_info.pg_node, FALSE);
		return TRUE;
	} 
	else 
	{		
        /*
		Get a page and set taken.
		*/
		page_addr = vmm_get_page(task->id, (UINT32)pf.linear);
 
        /*
		Map page onto the process address space.
		*/
		if(pm_page_in(task->id, (ADDR)PG_ADDRESS(pf.linear), (ADDR)LINEAR2PHYSICAL(page_addr), 2, perms) != SUCCESS)
			pman_print_and_stop("Not good... ");
			
		/*
		Unmap page from PMAN linear address space. 
		*/
		vmm_unmap_page(task->id, (UINT32)PG_ADDRESS(pf.linear));

		task->vmm_info.page_count++;

		return FALSE;
	}	
}

/*
Page Fault Interrupt Handler
*/
void vmm_paging_interrupt_handler()
{
	for(;;)
	{
		UINT16 thread_id;

		if(vmm_handle_page_fault(&thread_id, 0) || sch_running() == 0xFFFF) 
		{
			/* Resume scheduler for thread was put on hold */
			run_thread(SCHED_THR);
		}
		else
		{
			/* Resume currently running thread */
			run_thread(sch_running());
		}
	}
}

/* 
This function will be invoked upon completion of a seek command 
issued by the page fault handler.
*/
INT32 vmm_elffile_seekend_callback(struct fsio_event_source *iosrc, INT32 ioret)
{
	struct pm_thread *thread, *curr_thr, *othr;
	struct pm_task *task, *t;
    
	thread = thr_get(iosrc->id);
    
	if(ioret != IO_RET_OK)
	{
        pman_print_dbg("PF: Seek End IOERR \n");
		vmm_page_ioerror(thread, FALSE);
		return 0;
	}

    vmm_check_threads_pg(&thread, FALSE);

    if(!thread)
        return 0;

	/* Read from File */
	thread->io_finished.callback = vmm_elffile_readend_callback;
	io_begin_read(iosrc, thread->vmm_info.read_size, (ADDR)((UINT32)thread->vmm_info.page_in_address + thread->vmm_info.page_displacement));

	return 1;
}

/*
Function invoked upon completion of a read command from the executable file.
*/
INT32 vmm_elffile_readend_callback(struct fsio_event_source *iosrc, INT32 ioret)
{
	struct pm_thread *thread, *curr_thr, *othr;
	struct pm_task *task, *t;

	thread = thr_get(iosrc->id);

	if(ioret != IO_RET_OK)
	{
        pman_print_dbg("PF: Read End IO Error \n");
		vmm_page_ioerror(thread, FALSE);
		return 0;
	}
    
    vmm_check_threads_pg(&thread, FALSE);

    if(!thread)
        return 1;
    
    /* Page in on process address space */
    pm_page_in(thread->task_id, (ADDR)PG_ADDRESS(thread->vmm_info.fault_address), (ADDR)LINEAR2PHYSICAL(thread->vmm_info.page_in_address), 2, thread->vmm_info.page_perms);
    
	/* Un set IOLCK eflags on the page */
	vmm_set_flags(thread->task_id, thread->vmm_info.page_in_address, TRUE, TAKEN_EFLAG_IOLOCK, FALSE);
	
	/* Remove page from pman address space and create assigned record. */
	vmm_unmap_page(thread->task_id, PG_ADDRESS(thread->vmm_info.fault_address));
    
    /* Wake threads waiting for this page */
	vmm_wake_pf_threads(thread);

	return 1;
}

void vmm_page_ioerror(struct pm_thread *thread, BOOL removeTBLTree)
{
    struct pm_thread *curr_thr;
    int task_id = thread->task_id;
    struct pm_task *task = tsk_get(thread->task_id), *t;
    rbnode n = thread->vmm_info.pg_node;
    rbnode *currnode = NULL, *on = NULL;

    /* Return page to vmm, for it hasn't been mapped yet and it wouldn't be freed. */
	if(thread->vmm_info.page_in_address)
    {
        vmm_put_page(thread->vmm_info.page_in_address);
        thread->vmm_info.page_in_address = NULL;
    }
    
    currnode = thread->vmm_info.pg_node.next;
    
    fatal_exception(thread->task_id, PG_IO_ERROR);

    // If there's a thread from other task waiting, we will not 
    // send an exception, we will wake it (it's here because of 
    // an smo operation and it'll fail).
    while(currnode != NULL)
    {
        on = currnode->next;
        
        curr_thr = thr_get(currnode->value2);

        if(curr_thr->task_id != task_id)
        {
            t = tsk_get(curr_thr->task_id);
            
            if(curr_thr->state == TSK_KILLED)
            {
                removeChildRedBlackTree(&task->vmm_info.wait_root, currnode);
                if(removeTBLTree)
                    removeChildRedBlackTree(&task->vmm_info.tbl_wait_root, &thread->vmm_info.tbl_node);
                thr_destroy_thread(thread->id);

                if(t->killed_threads == 0)
                    tsk_destroy(task);
            }
            else
            {
                // wake the thread
                curr_thr->flags &= ~(THR_FLAG_PAGEFAULT | THR_FLAG_PAGEFAULT_TBL);
                curr_thr->state = THR_WAITING;
			    sch_activate(curr_thr);
            }
        }
        else
        {   
            // this is the page fault task.. since we sent a fatal exception,
            // it'll be on killing state
            curr_thr->flags &= ~THR_FLAG_PAGEFAULT;
        }

        currnode = on;
    }

    removeRedBlackTree(&task->vmm_info.wait_root, &thread->vmm_info.pg_node);
    if(removeTBLTree)
        removeChildRedBlackTree(&task->vmm_info.tbl_wait_root, &thread->vmm_info.tbl_node);
    thread->flags &= ~THR_FLAG_PAGEFAULT;
}

/*
This function will remove all threads killed
and destroy it's task if it was also killed.
*/
void vmm_check_threads_pg(struct pm_thread **thr, BOOL removeTBLTree)
{
    struct pm_thread *thread = *thr, *athread, *curr_thr, *othr;
	struct pm_task *task, *t;
    int task_id = thread->task_id;
    ADDR pg_addr = thread->vmm_info.page_in_address;
    rbnode n = thread->vmm_info.pg_node;
    rbnode *currnode = NULL, *on = NULL;

    athread = thread;
    task = tsk_get(thread->vmm_info.fault_task);
    currnode = thread->vmm_info.pg_node.next;
    
	while(currnode)
	{
        curr_thr = thr_get(currnode->value2);
    
        on = currnode->next;
        
        if(curr_thr->state == TSK_KILLED)
        {
            /* Thread was killed! */
            curr_thr->flags &= ~(THR_FLAG_PAGEFAULT | THR_FLAG_PAGEFAULT_TBL);
		     
			if(on)
            {
                othr = thr_get(on->value2);
                othr->vmm_info.page_in_address = curr_thr->vmm_info.page_in_address;
            }

            if(athread == curr_thr)
                athread = othr;
               
            /* this thread was killed and does not belong to the page fault task */
		    removeChildRedBlackTree(&task->vmm_info.wait_root, &curr_thr->vmm_info.pg_node);
            if(removeTBLTree)
                removeChildRedBlackTree(&task->vmm_info.tbl_wait_root, &curr_thr->vmm_info.tbl_node);
            
            if(curr_thr->task_id != task_id)
            {
                // destroy the thread
                t = tsk_get(curr_thr->task_id);
                thr_destroy_thread(curr_thr->id);
                
                if(t->state == TSK_KILLED && t->killed_threads == 0)
                    tsk_destroy(t);
            }   
        }
        else if(task->state == TSK_KILLED && curr_thr->task_id != task_id)
        {
            // if the page fault task was killed, wake all other threads 
            // from other tasks
            curr_thr->flags &= ~(THR_FLAG_PAGEFAULT | THR_FLAG_PAGEFAULT_TBL);
		    curr_thr->state = THR_WAITING;
	        sch_activate(curr_thr);
        }

        currnode = on;
	}
    
    // if the original task was killed, then destroy it
    if(task->state == TSK_KILLED && task->killed_threads == 0)
	{
        thr_destroy_thread(thread->id);

        removeRedBlackTree(&task->vmm_info.wait_root, &thread->vmm_info.pg_node); 
        if(removeTBLTree)
            removeRedBlackTree(&task->vmm_info.tbl_wait_root, &thread->vmm_info.tbl_node);

        if(task->killed_threads == 0 && task->state == TSK_KILLED)
                tsk_destroy(task);
                
        *thr = NULL;
        return;
	}

    if(athread == NULL && pg_addr)
    {
        vmm_put_page(pg_addr);
    }
        
    *thr = athread;
}

/*
Re-enable Threads Waiting for the same page.
// vmm_check_threads_pg must be invoked before invoking this function.
*/
void vmm_wake_pf_threads(struct pm_thread *thread)
{
	struct pm_thread *curr_thr = NULL;
    struct pm_task *task = NULL;
    rbnode *currnode = NULL, *on = NULL;

	/* First things first: Unblock the thread and reactivate it. */
	thread->state = THR_WAITING;	
	thread->flags &= ~(THR_FLAG_PAGEFAULT | THR_FLAG_PAGEFAULT_TBL);

	sch_activate(thread);

	/* 
	Go through waiting threads and enable all threads waiting for the same page.
	(pm_page_in is not necesary for the task is the same) 
	*/
    currnode = thread->vmm_info.pg_node.next;
    
	while(currnode != NULL)
	{
        curr_thr = thr_get(currnode->value2);
        on = currnode->next;

        curr_thr->flags &= ~THR_FLAG_PAGEFAULT;
		curr_thr->state = THR_WAITING;
	    sch_activate(curr_thr);

		currnode = on;
	}

    task = tsk_get(thread->vmm_info.fault_task);
    removeRedBlackTree(&task->vmm_info.wait_root, &thread->vmm_info.pg_node); // remove al threads waiting for the page from the tree

	/* 
		If there are no threads on the vmm_info.swaptbl_next list unlock the table.
	*/
    rbnode *n = searchRedBlackTree(&task->vmm_info.tbl_wait_root, TBL_ADDRESS(thread->vmm_info.fault_address));
	
	if(n)
	{
		/* remove IOLCK from the table */
		if(thread->vmm_info.tbl_node.next == NULL)
            vmm_set_flags(thread->task_id, (ADDR)PHYSICAL2LINEAR(vmm_get_tbl_physical(thread->task_id, thread->vmm_info.fault_address)), TRUE, TAKEN_EFLAG_IOLOCK, FALSE);
        removeRedBlackTree(&task->vmm_info.tbl_wait_root, &thread->vmm_info.tbl_node); // remove al threads waiting for the page from the tree
        thread->vmm_info.tbl_node.next = NULL;
	}
}

